---
title: "Data Mining - PS2"
author: "Chia Sheng Tu"
date: "2023-02-18"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(dplyr)
library(pROC)
library(class)
library(lattice)
library(mosaic)
```
## Problem 1: Saratoga House Prices

```{r Number 1, echo=FALSE, include=FALSE, warning=FALSE}

set.seed(123)
#Data Wrangling 
#split the data
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

#Normalize
Strain = model.matrix(~ . - (price +sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_train)
Stest = model.matrix(~ . - (price+ sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_test)

#Training and testing set responses
Xtrain = saratoga_train$price
Xtest = saratoga_test$price

#Rescale:
scale_train = apply(Strain, 2, sd)  # calculate std dev for each column
Xtilde_train = scale(Strain, scale = scale_train)
Xtilde_test = scale(Stest, scale = scale_train)  # use the training set scales!

#Fit the full model 
full.model <- lm(price ~., data = saratoga_train)

#Stepwise regression model
step.model <- step(full.model, direction = c("both"))
step.model = lm(price ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms  + 
                  heating + waterfront + newConstruction + centralAir, data = saratoga_train)
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data= saratoga_train)
rmse(lm_medium,saratoga_test)

#Calculate simulated RSME
rmse_knn = do(10)*{
  #split the data
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                   fireplaces + bathrooms + rooms + heating + fuel + centralAir, data= saratoga_train)
  
  step.model = lm(price ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms  + 
                    heating + waterfront + newConstruction + centralAir, data = saratoga_train)
  
  model_errors = c(rmse(lm_medium, saratoga_test), rmse(step.model,saratoga_test))
  
  model_errors
}

#KNN MODEL
rmse_knn = do(10)*{
  #Split the data
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  #Normalize
  
  Strain = model.matrix(~ . - (price +sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_train)
  Stest = model.matrix(~ . - (price+ sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_test)
  
  #Training and testing set responses
  Xtrain = saratoga_train$price
  Xtest = saratoga_test$price
  
  #Rescale:
  scale_train = apply(Strain, 2, sd)  # calculate std dev for each column
  Xtilde_train = scale(Strain, scale = scale_train)
  Xtilde_test = scale(Stest, scale = scale_train)  # use the training set scales!
  
  #Run the KNN model
  ctrl <- trainControl(method="repeatedcv", number = 10, repeats = 3)
  knnfit <- train(Xtilde_train,
                  Xtrain,
                  method = "knn",
                  trControl = ctrl,
                  tunelenth = 10)
  #knnfit
  
  y_predict <- predict(knnfit, Xtilde_test)
  
  
  knn_errors = c(RMSE(Xtest, y_predict))
  
}
finalKNNrmse = colMeans(rmse_knn)


```
The linear model which outperformed the medium linear model is: 
price = lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms  + heating + waterfront + newConstruction + centralAir  (by using Stepwise regression)

By using a cross validated RMSE, we found that the linear medium model had a RMSE of 68077.25 and our chosen linear model had a RMSE of 63437.27. The KNN model had  a RMSE of 66280.28 which was selected by using repeated cross validation and then refitted to the testing set. Our chosen linear model is the best at predicting market values for properties in Saratoga (do better at achieving lower out-of-sample mean-squared error). For a taxing authority, it is clear that there are some important factors in determining property value compared to the medium model, including Land Value, Waterfront Property and whether a house is a new construction. 


## Problem 2: Classification and retrospective sampling

```{r problem 2_bar_chart, message=FALSE, echo=FALSE, warning=FALSE}
credit <- read.csv("Data\\german_credit.csv")
#Group the data by "history" 
#calculate average default by each credit history class 
default_mean <- credit %>%
  group_by(history) %>%
  summarize(mean_default = mean(Default))

#Bar plot
ggplot(default_mean, aes(x = history, y = mean_default) ) + geom_bar(stat = "identity") + labs(y = "Probability of Default", 
       x = "Class of Credit History",
       title = "Probability of Default by Credit History", 
       ) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


```


According to the bar chart above, borrowers whose credit ratings classified as “good” have the highest possibility (60%) to default on the loan. On the other hand, borrowers with “poor” and “terrible” credit ratings, however, have lower default probability than borrowers with “good” credit ratings.


```{r problem 2_model, message=FALSE, echo=FALSE, warning=FALSE}
#Split the train/testing data
credit_split =  initial_split(credit, prop=0.8)
credit_train = training(credit_split)
credit_test  = testing(credit_split)

#Build the model 
logit_default = glm(Default~duration + amount + installment + age + history + purpose + foreign, data = credit_train, family = binomial)
knitr::kable(coef(logit_default) %>% round(2))

#Fit in a logistic regression model
model <- glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data = credit_train, family = binomial)

#Model summary
summary(model)


```

The bank could not predict the outcome accurately based on class of credit history. Based on information above, the coefficients of "poor" and "terrible" credit history are both negative, saying that borrowers with bad credit history have lower chance to default on the loan.  

For this reason, it may not be appropriate to use this data set alone for building a predictive model of "Default" to screen prospective borrowers into "high" versus "low" probability of default. The data is full of defaulted loans, creating biased estimator or even selection bias. The bank should use random sampling data than this "case-control" designed data.


