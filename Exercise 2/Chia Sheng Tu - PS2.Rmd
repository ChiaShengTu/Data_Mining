---
title: "Data Mining - PS2Q2"
author: "Chia Sheng Tu"
date: "2023-02-18"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(dplyr)
library(pROC)
library(class)
library(lattice)
```
## Problem 1: Saratoga House Prices
```{r Number 1, echo=FALSE, include=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(rsample)  
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(mosaic)
library(class)
library(lattice)
set.seed(123)
# Data Wrangling 
#split the data
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
#Normalize

Strain = model.matrix(~ . - (price +sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_train)
Stest = model.matrix(~ . - (price+ sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_test)
# training and testing set responses
Xtrain = saratoga_train$price
Xtest = saratoga_test$price
#now rescale:
scale_train = apply(Strain, 2, sd)  # calculate std dev for each column
Xtilde_train = scale(Strain, scale = scale_train)
Xtilde_test = scale(Stest, scale = scale_train)  # use the training set scales!
# Fit the full model 
full.model <- lm(price ~., data = saratoga_train)
# Stepwise regression model
step.model <- step(full.model, direction = c("both"))
step.model = lm(price ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms  + 
                  heating + waterfront + newConstruction + centralAir, data = saratoga_train)
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data= saratoga_train)
rmse(lm_medium,saratoga_test)
#get simulated RSME
rmse_knn = do(10)*{
  #split the data
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                   fireplaces + bathrooms + rooms + heating + fuel + centralAir, data= saratoga_train)
  
  step.model = lm(price ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms  + 
                    heating + waterfront + newConstruction + centralAir, data = saratoga_train)
  
  model_errors = c(rmse(lm_medium, saratoga_test), rmse(step.model,saratoga_test))
  
  model_errors
}

#KNN MODEL
rmse_knn = do(10)*{
  #split the data
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  #Normalize
  
  Strain = model.matrix(~ . - (price +sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_train)
  Stest = model.matrix(~ . - (price+ sewer + waterfront + landValue + newConstruction) - 1, data=saratoga_test)
  
  # training and testing set responses
  Xtrain = saratoga_train$price
  Xtest = saratoga_test$price
  
  #now rescale:
  scale_train = apply(Strain, 2, sd)  # calculate std dev for each column
  Xtilde_train = scale(Strain, scale = scale_train)
  Xtilde_test = scale(Stest, scale = scale_train)  # use the training set scales!
  
  #run the KNN model
  ctrl <- trainControl(method="repeatedcv", number = 10, repeats = 3)
  knnfit <- train(Xtilde_train,
                  Xtrain,
                  method = "knn",
                  trControl = ctrl,
                  tunelenth = 10)
  #knnfit
  
  y_predict <- predict(knnfit, Xtilde_test)
  
  
  knn_errors = c(RMSE(Xtilde_test, y_predict))
  
}
finalKNNrmse = colMeans(rmse_knn)
```
The linear model outperformed the medium linear model by: Price = lotSize + age + land value + living area + bedroom + bathroom + room + heating + waterfront + new building + central air, which was found using stepwise regression. 

Using the cross-validated RMSE, we found that the RMSE for the linear medium model was 69966, while the RMSE for the linear model we selected was 63280. The RMSE for the KNN model was 69919, which was selected by repeated cross-validation and then modded to the test set. This means that the linear model we chose was the best model for predicting the market value of Saratoga homes. For the tax authorities, it is clear that there are important factors in determining the value of a property compared to the medium model: the value of the land, the waterfront property, and finally whether the house is new construction. 




## Problem 2: Classification and retrospective sampling

```{r problem 2_bar_chart, message=FALSE, echo=FALSE, warning=FALSE}
credit <- read.csv("Data\\german_credit.csv")
default_data <- credit %>%
  group_by(history) %>%
  summarize(mean_default = mean(Default))

# Create the bar plot
ggplot(default_data, aes(x = history, y = mean_default) ) + geom_bar(stat = "identity") + labs(y = "Probability of Default", 
       x = "Class of Credit History",
       title = "Probability of Default by Credit History", 
       ) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


```


According to the bar chart above, borrowers whose credit ratings classified as “good” have the highest possibility (60%) to default on the loan. On the other hand, borrowers with “poor” and “terrible” credit ratings, however, have lower default probability than borrowers with “good”.


```{r problem 2, message=FALSE, echo=FALSE, warning=FALSE}
# split the train/test data
credit_split =  initial_split(credit, prop=0.8)
credit_train = training(credit_split)
credit_test  = testing(credit_split)
# building the model 
logit_default = glm(Default~duration + amount + installment + age + history + purpose + foreign, data = credit_train, family = binomial)
knitr::kable(coef(logit_default) %>% round(2))

# Fit a logistic regression model
model <- glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data = credit_train, family = binomial)

# Print the model summary
summary(model)

```

The bank could not predict the outcome accurately based on class of credit history. Based on information above, the coefficient of "poor" and "terrible" credit history are negative, saying that people with bad credit history have lower chance to default on the loan.  

For this reason, it may not be appropriate to use this data set alone for building a predictive model of "Default" to screen prospective borrowers into "high" versus "low" probability of default. The data is full of defaulted loans, creating biased estimator or even selection bias. The bank should use random sampling data than this "case-control" designed data.


